{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# Requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# For decoding and reading base64 image\n",
    "import base64\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# For extracting text from image\n",
    "import cv2\n",
    "import pytesseract\n",
    "import io\n",
    "# For simulating browser bahaviour for solving captchas\n",
    "from selenium import webdriver\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "# pd.reset_option()\n",
    "\n",
    "enable_proxy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_vin(encoded_vin):\n",
    "    im = Image.open(BytesIO(base64.b64decode(encoded_vin))).resize((400, 50))\n",
    "    return pytesseract.image_to_string(im)\n",
    "\n",
    "def getProxies():\n",
    "    pr_list = []\n",
    "    counter = 0\n",
    "    for i, row in df_proxy[df_proxy.raw_ip.isin(success_proxy)].iterrows():\n",
    "        print(row.ip, counter, df_proxy[df_proxy.raw_ip.isin(success_proxy)].shape[0])\n",
    "        counter += 1\n",
    "        proxy = {'https': f'https://{row.ip}'}\n",
    "        try:\n",
    "            r = requests.get('https://api.ipify.org/', timeout=2, proxies=proxy, headers=headers)\n",
    "            if row.ip.startswith(r.text):\n",
    "                r = requests.get('https://autoplius.lt/skelbimai/volvo-xc60-2-4-l-visureigis-2009-dyzelinas-9742229.html', timeout=2, proxies=proxy, headers=headers)\n",
    "                pr_list.append(row.ip)\n",
    "        except:\n",
    "            continue\n",
    "    return pr_list\n",
    "\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 5.1; rv:9.0.1) Gecko/20100101 Firefox/9.0.1\"\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update(\n",
    "    {\n",
    "        'User-Agent': USER_AGENT,\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "proxy_list = requests.get('https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list.txt')\n",
    "proxies = re.findall('[0-9]+.[0-9]+.[0-9]+.[0-9]+:[0-9]+.*\\n', proxy_list.text)\n",
    "ip_regex = '[0-9]+.[0-9]+.[0-9]+.[0-9]+:[0-9]+'\n",
    "df_proxy = pd.DataFrame([{'ip': re.findall(ip_regex, proxy)[0], 'country': proxy.split(' ')[1].split('-')[0], 'anonimity': proxy.split(' ')[1].split('-')[1]} for proxy in proxies])\n",
    "pool_counter = 0\n",
    "what_is_my_ip = 'https://api.ipify.org/'\n",
    "\n",
    "status_proxy = requests.get(\"https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-status.txt\")\n",
    "success_proxy = [re.sub(':.*', '', proxy) for proxy in status_proxy.text.split('\\n') if proxy.endswith('success')]\n",
    "df_proxy['raw_ip'] = df_proxy.ip.apply(lambda x: re.sub(\":.*\", '', x))\n",
    "\n",
    "df_urls = pd.read_csv('output.csv', lineterminator='\\n')\n",
    "\n",
    "if enable_proxy:\n",
    "    tst_success = getProxies() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all available urls into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_links():\n",
    "    sitemap = requests.get('https://autoplius.lt/xml_sitemap/index.xml', headers=headers, timeout=30)\n",
    "    soup = BeautifulSoup(sitemap.text, \"html.parser\")\n",
    "    ads_sitemap = [tag.text for tag in soup.findAll('loc') if 'ann_list' in tag.text]\n",
    "\n",
    "    product_urls = []\n",
    "    last_updated = []\n",
    "    priority = []\n",
    "    for sitemap_url in ads_sitemap:\n",
    "        print(sitemap_url)\n",
    "        sitemap_r = requests.get(sitemap_url, headers=headers, timeout=30)\n",
    "        links_soup = BeautifulSoup(sitemap_r.text, \"html.parser\")\n",
    "\n",
    "        product_urls.extend([tag.find(\"xhtml:link\", {'hreflang': 'en'})['href'] for tag in links_soup.findAll('url')][0::4])\n",
    "        last_updated.extend([tag.find('lastmod').text for tag in links_soup.findAll('url')][0::4])\n",
    "        priority.extend([tag.find('priority').text for tag in links_soup.findAll('url')][0::4])\n",
    "\n",
    "    return pd.DataFrame({'url': product_urls, 'last_updated': last_updated, 'ad_priority': priority})\n",
    "    #     sitemaps_links.extend({'url': product_urls, 'last_updated': last_updated, 'ad_priority': priority}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape each url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def solve_captcha():\n",
    "    driver.get('https://autoplius.lt/')\n",
    "    driver.get_screenshot_as_file('captcha.png') \n",
    "    input_elem = driver.find_element_by_xpath(\"//input[@id='code']\")\n",
    "    img = Image.open(\"captcha.png\")\n",
    "    area = (900, 510, 1025, 560)\n",
    "    cropped_img = img.crop(area)\n",
    "    input_elem.send_keys(read_captcha(cropped_img))\n",
    "    driver.find_element_by_xpath(\"//button[@type='submit']\").click()\n",
    "\n",
    "\n",
    "def read_captcha(image):\n",
    "#     image = Image.open(io.BytesIO(requests.get('https://en.autoplius.lt/utility/captcha/text').content))\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Morph open to remove noise and invert image\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    invert = 255 - opening\n",
    "\n",
    "    # Perform text extraction\n",
    "    data = re.sub('[ ]*', '', pytesseract.image_to_string(invert, lang='eng', config='--psm 6')).upper()\n",
    "    res = re.findall('[a-zA-Z]{4}', data)[0] if len(re.findall('[a-zA-Z]{4}', data)) else 'ABCD'\n",
    "    return res\n",
    "\n",
    "\n",
    "def print_exception(column_name, e):\n",
    "    print(column_name, e)\n",
    "\n",
    "    \n",
    "'''\n",
    "inspect_exceptions = True: display exceptions message if something triggers except block\n",
    "inspect_exceptions = False: ignore exceptions message if something triggers except block\n",
    "'''\n",
    "inspect_exceptions = False\n",
    "\n",
    "# Driver for solving captchas\n",
    "driver = webdriver.Firefox(executable_path='/home/domantas/Documents/selenium_drivers/geckodriver')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "error_log = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "for i, row in df_urls[(df_urls.scrape_date.isnull()) | (df_urls.breadcrumbs == '[]')].iterrows():\n",
    "    try:\n",
    "        if enable_proxy:\n",
    "            try:\n",
    "                proxy = {'https': f'https://{random.choice(tst_success)}'}\n",
    "                r = requests.get(row.url, proxies=proxy, headers=headers, timeout=2)\n",
    "            except:\n",
    "                try:\n",
    "                    r = requests.get(row.url, headers=headers, timeout=2)\n",
    "                except:\n",
    "                    continue\n",
    "        else:\n",
    "            r = requests.get(row.url, headers=headers, timeout=10)\n",
    "        if r.status_code == 429:\n",
    "            solve_captcha()\n",
    "            print('BLOCKED', counter)\n",
    "#             break\n",
    "        prod_soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        if counter % 50 == 0:\n",
    "            print(f\"{counter}/{df_urls.shape[0]} | {df_urls[(~df_urls.scrape_date.isnull()) & (df_urls.breadcrumbs != '[]')].shape[0]} / {df_urls.shape[0]} | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ||| {row.url}\")\n",
    "        try:\n",
    "            df_urls.loc[i, 'title'] = prod_soup.find(attrs = {'class': 'page-title'}).text.replace('\\n', '')\n",
    "        except Exception as e:\n",
    "            print_exception('title', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'title'] = ''\n",
    "\n",
    "        try:\n",
    "            df_urls.loc[i, 'price'] = re.sub('(\\n)|([ ]+)', '', prod_soup.find(attrs = {'class': 'price'}).text)\n",
    "        except Exception as e:\n",
    "            print_exception('price', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'price'] = 'Price is negotiable'\n",
    "\n",
    "        try:\n",
    "            df_urls.loc[i, 'phone'] = re.sub('(\\n)|([ ]+)', '', prod_soup.find(attrs = {'class': 'seller-phone-number'}).text)\n",
    "        except Exception as e:\n",
    "            print_exception('phone', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'phone'] = ''\n",
    "\n",
    "        try:\n",
    "            df_urls.loc[i, 'description'] = prod_soup.findAll('div', {'class': 'announcement-description'})[0].text\n",
    "        except Exception as e:\n",
    "            print_exception('description', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'description'] = ''\n",
    "\n",
    "        try:\n",
    "            df_urls.loc[i, 'contact_name'] = prod_soup.find('div', {'class': 'seller-contact-name'}).text\n",
    "        except Exception as e:\n",
    "            print_exception('contact_name', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'contact_name'] = ''\n",
    "\n",
    "        try:\n",
    "            df_urls.loc[i, 'contact_location'] = re.sub('(\\n)|([ ]{2,})', '', prod_soup.find('div', {'class': 'seller-contact-location'}).text)\n",
    "        except Exception as e:\n",
    "            print_exception('contact_location', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'contact_location'] = ''\n",
    "\n",
    "        try:\n",
    "            df_urls.loc[i, 'article_id'] = re.sub('(\\n)|([ ]+)', '', prod_soup.find('li', {'class': 'announcement-id'}).text)\n",
    "        except Exception as e:\n",
    "            print_exception('article_id', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'article_id'] = ''\n",
    "\n",
    "        try:\n",
    "            df_urls.loc[i, 'image'] = str([re.sub('ann_[0-9]', 'ann_3', re.findall(\"https://autoplius-img.dgn.lt/.*.jpg\", tag['style'])[0]) for tag in prod_soup.findAll('div', {'class': 'thumbnail'})])\n",
    "        except Exception as e:\n",
    "            print_exception('image', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'image'] = ''\n",
    "\n",
    "        try:\n",
    "            df_urls.loc[i, 'breadcrumbs'] = str([re.sub('(\\n)|([ ]+)', '', tag.text) for tag in prod_soup.findAll('li', {'class': 'crumb'})])\n",
    "        except Exception as e:\n",
    "            print_exception('breadcrumbs', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'breadcrumbs'] = ''\n",
    "\n",
    "        df_urls.loc[i, 'scrape_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        try:\n",
    "            params_label = [re.sub('(\\n)|([ ]{2,})', '', tag.text) for tag in prod_soup.findAll(attrs = {'class': 'parameter-label'})]\n",
    "            params_value = [re.sub('(\\n)|([ ]{2,})', '', tag.text) for tag in prod_soup.findAll(attrs = {'class': 'parameter-value'})]\n",
    "            params = list(zip(params_label, params_value))\n",
    "            for p in params:\n",
    "                df_urls.loc[i, p[0]] = p[1]\n",
    "\n",
    "            # Extract VIN number if exists\n",
    "            if len(re.findall('data:image/png;base64, .*\"', r.text)):\n",
    "                encoded_vin = re.sub('data:image/png;base64, ', '', re.findall('data:image/png;base64, .*\"', r.text)[0][:-1])\n",
    "                df_urls.loc[i, 'VIN number'] = decode_vin(encoded_vin)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_log = error_log.append([{'params': e, 'url': row.url}])\n",
    "\n",
    "        try:\n",
    "            for section in prod_soup.findAll('div', {'class': 'section'}):\n",
    "                if len(section.findAll('div', {'class': 'feature-row'})):\n",
    "                    feature_header = re.sub('(\\n)|([ ]{2,})', '', section.find('div', {'class': 'heading'}).text)\n",
    "                    df_urls.loc[i, feature_header] = [section]\n",
    "\n",
    "        except Exception as e:\n",
    "            error_log = error_log.append([{'features': e, 'url': row.url}])\n",
    "        try:\n",
    "            df_urls.loc[i, 'memorized'] = re.findall('[0-9]+', prod_soup.findAll('span', {'class': 'bookmark-ico'})[0].parent.text)[0]\n",
    "        except Exception as e:\n",
    "            print_exception('memorized', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'memorized'] = ''\n",
    "        try:\n",
    "            df_urls.loc[i, 'update_timestamp'] = [tag.text for tag in prod_soup.findAll('span', {'class': 'bar-item'}) if 'Updated' in tag.text][0]\n",
    "        except Exception as e:\n",
    "            print_exception('update_timestamp', e) if inspect_exceptions else ''\n",
    "            df_urls.loc[i, 'update_timestamp'] = ''\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0: \n",
    "            df_urls.to_csv('output.csv', index=False)\n",
    "    except Exception as e:\n",
    "        print(f'Failed: {row.url}', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        print(e)\n",
    "\n",
    "driver.close()\n",
    "df_urls.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
