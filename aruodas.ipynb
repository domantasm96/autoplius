{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://autoplius.lt/xml_sitemap/ann_list/1.xml 2020-01-23 23:13:33\n",
      "0/5000 | 2020-01-23 23:13:45\n",
      "1/5000 | 2020-01-23 23:13:45\n",
      "2/5000 | 2020-01-23 23:13:46\n",
      "3/5000 | 2020-01-23 23:13:46\n",
      "4/5000 | 2020-01-23 23:13:46\n",
      "5/5000 | 2020-01-23 23:13:46\n",
      "6/5000 | 2020-01-23 23:13:47\n",
      "7/5000 | 2020-01-23 23:13:47\n",
      "8/5000 | 2020-01-23 23:13:47\n",
      "9/5000 | 2020-01-23 23:13:48\n",
      "10/5000 | 2020-01-23 23:13:48\n",
      "11/5000 | 2020-01-23 23:13:48\n",
      "12/5000 | 2020-01-23 23:13:49\n",
      "13/5000 | 2020-01-23 23:13:49\n",
      "14/5000 | 2020-01-23 23:13:50\n",
      "15/5000 | 2020-01-23 23:13:50\n",
      "16/5000 | 2020-01-23 23:13:50\n",
      "17/5000 | 2020-01-23 23:13:51\n",
      "18/5000 | 2020-01-23 23:13:51\n",
      "19/5000 | 2020-01-23 23:13:51\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "error_log = pd.DataFrame()\n",
    "\n",
    "sitemap = requests.get('https://autoplius.lt/xml_sitemap/index.xml')\n",
    "soup = BeautifulSoup(sitemap.text, \"html.parser\")\n",
    "ads_sitemap = [tag.text for tag in soup.findAll('loc') if 'ann_list' in tag.text]\n",
    "for sitemap_url in ads_sitemap:\n",
    "    print(sitemap_url, datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    sitemap_link = requests.get(sitemap_url)\n",
    "    links_soup = BeautifulSoup(sitemap_link.text, \"html.parser\")\n",
    "    products = list(set([(tag.find(\"xhtml:link\", {'hreflang': 'en'})['href'], tag.find('lastmod').text, tag.find('priority').text) for tag in links_soup.findAll('url')]))\n",
    "    items = []\n",
    "    for i, url in enumerate(products):\n",
    "        print(f\"{i}/{len(products)} | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        r = requests.get(url[0])\n",
    "        prod_soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        data = {}\n",
    "        try:\n",
    "            data['title'] = prod_soup.find(attrs = {'class': 'page-title'}).text.replace('\\n', '')\n",
    "        except:\n",
    "            data['title'] = ''\n",
    "        \n",
    "        try:\n",
    "            data['price'] = re.sub('(\\n)|([ ]+)', '', prod_soup.find(attrs = {'class': 'price'}).text)\n",
    "        except:\n",
    "            data['price'] = 'nagotiate'\n",
    "        \n",
    "        try:\n",
    "            data['phone'] = re.sub('(\\n)|([ ]+)', '', prod_soup.find(attrs = {'class': 'seller-phone-number'}).text)\n",
    "        except:\n",
    "            data['phone'] = ''\n",
    "        \n",
    "        try:\n",
    "            data['description'] = prod_soup.findAll('div', {'class': 'announcement-description'})[0].text\n",
    "        except:\n",
    "            data['description'] = ''\n",
    "        \n",
    "        try:\n",
    "            data['contact_name'] = prod_soup.find('div', {'class': 'seller-contact-name'}).text\n",
    "        except:\n",
    "            data['contact_name'] = ''\n",
    "            \n",
    "        try:\n",
    "            data['contact_location'] = re.sub('(\\n)|([ ]{2,})', '', prod_soup.find('div', {'class': 'seller-contact-location'}).text)\n",
    "        except:\n",
    "            data['contact_location'] = ''\n",
    "        \n",
    "        data['sitemap_link'] = sitemap_url\n",
    "        data['url'] = url\n",
    "        data['scrape_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        try:\n",
    "            params_label = [re.sub('(\\n)|([ ]{2,})', '', tag.text) for tag in prod_soup.findAll(attrs = {'class': 'parameter-label'})]\n",
    "            params_value = [re.sub('(\\n)|([ ]{2,})', '', tag.text) for tag in prod_soup.findAll(attrs = {'class': 'parameter-value'})]\n",
    "            params = list(zip(params_label, params_value))\n",
    "            for p in params:\n",
    "                data[p[0]] = p[1]\n",
    "        except Exception as e:\n",
    "            error_log = error_log.append([{'params': e, 'url': url}])\n",
    "        \n",
    "        try:\n",
    "            features = []\n",
    "            for row in prod_soup.findAll('div', {'class': 'feature-row'}):\n",
    "                feature_label = [re.sub('(\\n)|([ ]{2,})', '', tag.text) for tag in row.findAll('div', {'class': 'feature-label'})]\n",
    "                feature_value = [re.sub('(\\n)|([ ]{2,})', '', tag.text) for tag in row.findAll('span', {'class': 'feature-item'})]\n",
    "                features.append({feature_label[0]: feature_value})\n",
    "\n",
    "            for row in features:\n",
    "                data.update(row)\n",
    "        except Exception as e:\n",
    "            error_log = error_log.append([{'features': e, 'url': url}])\n",
    "        try:\n",
    "            memorized = re.findall('[0-9]+ lankytojai', prod_soup.findAll('div', {'class': 'bookmark-stats-bar'})[0].text)[0]\n",
    "        except Exception as e:\n",
    "            memorized = ''\n",
    "        try:\n",
    "            update_timestamp = [tag.text for tag in prod_soup.findAll('span', {'class': 'bar-item'}) if 'Atnaujintas' in tag.text][0]\n",
    "        except Exception as e:\n",
    "            update_timestamp = ''\n",
    "\n",
    "        data['memorized'] = memorized\n",
    "        data['update_timestamp'] = update_timestamp\n",
    "        \n",
    "        data['last_updated'] = url[1]\n",
    "        data['adv_priority'] = url[2]\n",
    "        \n",
    "        items.append(data)\n",
    "        \n",
    "    df = df.append(items)\n",
    "    df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
